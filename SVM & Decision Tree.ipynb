{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8020d3ae",
   "metadata": {},
   "source": [
    "# SVM & Decision Tree\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be39033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11374e",
   "metadata": {},
   "source": [
    "## Import Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa87f188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set : (20392, 1) (20392, 99)\n",
      "Test Set  : (8176, 1) (8176, 99)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('Data/X_train_engineered.csv')\n",
    "X_test = pd.read_csv('Data/X_test_engineered.csv')\n",
    "Y_train = pd.read_csv('Data/y_train_undersampled_data.csv')\n",
    "Y_test = pd.read_csv('Data/y_test.csv')\n",
    "\n",
    "print(\"Train Set :\", Y_train.shape, X_train.shape)\n",
    "print(\"Test Set  :\", Y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2929e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for calculating evaluation scores\n",
    "def evaluate(Y_train, Y_train_pred, Y_test, Y_test_pred, version, train_metrics, test_metrics):\n",
    "    train_metric = {\n",
    "    \"Version\": version,\n",
    "    \"Accuracy\": accuracy_score(Y_train, Y_train_pred),\n",
    "    \"Precision\": precision_score(Y_train, Y_train_pred),\n",
    "    \"Recall\": recall_score(Y_train, Y_train_pred),\n",
    "    \"F1 Score\": f1_score(Y_train, Y_train_pred)\n",
    "    }\n",
    "\n",
    "    test_metric = {\n",
    "        \"Version\": version,\n",
    "        \"Accuracy\": accuracy_score(Y_test, Y_test_pred),\n",
    "        \"Precision\": precision_score(Y_test, Y_test_pred),\n",
    "        \"Recall\": recall_score(Y_test, Y_test_pred),\n",
    "        \"F1 Score\": f1_score(Y_test, Y_test_pred)\n",
    "    }\n",
    "\n",
    "    # Save to overall metrics dataframe for comparison later\n",
    "    if len(train_metrics)==0:\n",
    "        train_metrics = pd.DataFrame.from_records([train_metric])\n",
    "        test_metrics = pd.DataFrame.from_records([test_metric])\n",
    "    else:\n",
    "        train_metrics = pd.concat([train_metrics, pd.DataFrame.from_records([train_metric])], ignore_index = True)\n",
    "        test_metrics = pd.concat([test_metrics, pd.DataFrame.from_records([test_metric])], ignore_index = True)\n",
    "\n",
    "    # Calculate general metrics for the train set\n",
    "    print(\"**Training Set Metrics**\")\n",
    "    print(\"Accuracy \\t:\", train_metric[\"Accuracy\"])\n",
    "    print(\"Precision \\t:\", train_metric[\"Precision\"])\n",
    "    print(\"Recall \\t\\t:\", train_metric[\"Recall\"])\n",
    "    print(\"F1 Score \\t:\", train_metric[\"F1 Score\"])\n",
    "\n",
    "    print() # New Line\n",
    "\n",
    "    # Calculate general metrics for the test set\n",
    "    print(\"**Test Set Metrics**\")\n",
    "    print(\"Accuracy \\t:\", test_metric[\"Accuracy\"])\n",
    "    print(\"Precision \\t:\", test_metric[\"Precision\"])\n",
    "    print(\"Recall \\t\\t:\", test_metric[\"Recall\"])\n",
    "    print(\"F1 Score \\t:\", test_metric[\"F1 Score\"])\n",
    "    \n",
    "    return train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c37f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dataframe to store the results from different versions\n",
    "train_metrics = pd.DataFrame(columns=['Version', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "test_metrics = pd.DataFrame(columns=['Version', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec270638",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaace06",
   "metadata": {},
   "source": [
    "## Feature Selection for SVM\n",
    "\n",
    "We are applying three techniques for SVM feature selection:\n",
    "1. Forward Feature Selection using SVM <br>\n",
    "Forward Feature Selection involves iteratively adding features to the model, starting with one and gradually increasing the feature set. With each iteration, the model's performance is evaluated using metrics like accuracy or R-squared value. This method helps identify the most relevant features by considering them one at a time.\n",
    "\n",
    " \n",
    "2. Backward Feature Selection using SVM <br>\n",
    "Backward Feature Selection begins with all features included and progressively removes one feature at a time, evaluating performance at each step. By iteratively eliminating less relevant features, this method aims to identify the most important subset of features for the model. \n",
    "\n",
    "\n",
    "3. Recursive Feature selection using SVM <br>\n",
    "Recursive Feature Selection prioritizes feature importance and iteratively removes the least important features to identify the most informative subset. It typically involves using feature ranking techniques in conjunction with the model to select the most relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47917fd1",
   "metadata": {},
   "source": [
    "#### Forward Feature Selection using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Initialize forward feature selector\n",
    "selector = SequentialFeatureSelector(svm, direction='forward', scoring='f1', cv=5, n_features_to_select='auto', tol=0.01)\n",
    "# Perform forward feature selection\n",
    "selector.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Get selected feature indices\n",
    "selected_indices_forward = selector.get_support(indices=True)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_forward = X_train.columns[selected_indices_forward]\n",
    "\n",
    "# Print selected features\n",
    "print(\"Selected features:\", selected_features_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef8145",
   "metadata": {},
   "source": [
    "#### Backward Feature Selection using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Initialize forward feature selector\n",
    "selector = SequentialFeatureSelector(svm, direction='backward', scoring='f1', cv=5, n_features_to_select='auto', tol=0.01)\n",
    "# Perform forward feature selection\n",
    "selector.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Get selected feature indices\n",
    "selected_indices_backward = selector.get_support(indices=True)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_backward = X_train.columns[selected_indices_backward]\n",
    "\n",
    "# Print selected features\n",
    "print(\"Selected features:\", selected_features_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6166757",
   "metadata": {},
   "source": [
    "#### Recursive Feature selection using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12573a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Initialize RFE for SVM\n",
    "rfe = RFE(estimator=svm, scoring='f1')\n",
    "\n",
    "# Fit RFE\n",
    "rfe.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Get selected feature indices\n",
    "selected_indices_recursive = rfe.get_support(indices=True)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_recursive = X_train.columns[selected_indices_recursive]\n",
    "\n",
    "# Print selected features\n",
    "print(\"Selected features:\", selected_features_recursive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474da2c7",
   "metadata": {},
   "source": [
    "## SVM with Selected Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473d6b7",
   "metadata": {},
   "source": [
    "#### SVM with Forward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0db1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset training and test data with selected features\n",
    "X_train_selected = X_train[selected_features_forward]\n",
    "X_test_selected = X_test[selected_features_forward]\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train SVM classifier on the selected features\n",
    "svm.fit(X_train_selected, Y_train.values.ravel())\n",
    "\n",
    "# Predict using the trained SVM classifier\n",
    "Y_train_pred = svm.predict(X_train_selected)\n",
    "Y_test_pred = svm.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80178e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_forward, test_metrics_forward = evaluate(Y_train, Y_train_pred, Y_test, Y_test_pred, \"Feature Selected\", train_metrics, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a3362",
   "metadata": {},
   "source": [
    "#### SVM with Backward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a861056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset training and test data with selected features\n",
    "X_train_selected = X_train[selected_features_backward]\n",
    "X_test_selected = X_test[selected_features_backward]\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train SVM classifier on the selected features\n",
    "svm.fit(X_train_selected, Y_train.values.ravel())\n",
    "\n",
    "# Predict using the trained SVM classifier\n",
    "Y_train_pred = svm.predict(X_train_selected)\n",
    "Y_test_pred = svm.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615111c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_backward, test_metrics_backward = evaluate(Y_train, Y_train_pred, Y_test, Y_test_pred, \"Feature Selected\", train_metrics, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308066d8",
   "metadata": {},
   "source": [
    "#### SVM with Recursive Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c43e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset training and test data with selected features\n",
    "X_train_selected = X_train[selected_features_recursive]\n",
    "X_test_selected = X_test[selected_features_backward_recursive]\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train SVM classifier on the selected features\n",
    "svm.fit(X_train_selected, Y_train.values.ravel())\n",
    "\n",
    "# Predict using the trained SVM classifier\n",
    "Y_train_pred = svm.predict(X_train_selected)\n",
    "Y_test_pred = svm.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd957fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_recursive, test_metrics_recursive = evaluate(Y_train, Y_train_pred, Y_test, Y_test_pred, \"Feature Selected\", train_metrics, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588bc5af",
   "metadata": {},
   "source": [
    "## Hyperparameter-tuning for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f4595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c32eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b2a65ec",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2bc82",
   "metadata": {},
   "source": [
    "## Feature Selection for Decision Tree\n",
    "\n",
    "Decision tree inherently performs feature selection as part of its model training process. In scikit-learn, the feature importance provided by decision tree-based models is based on the Gini importance. Gini importance measures the total decrease in node impurity (often quantified using the Gini index) weighted by the probability of reaching that node during the construction of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac0b8fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Training Set Metrics**\n",
      "Accuracy \t: 0.9928893683797567\n",
      "Precision \t: 0.9960517224360873\n",
      "Recall \t\t: 0.9897018438603374\n",
      "F1 Score \t: 0.9928666305898558\n",
      "\n",
      "**Test Set Metrics**\n",
      "Accuracy \t: 0.7602739726027398\n",
      "Precision \t: 0.4141975308641975\n",
      "Recall \t\t: 0.3989298454221165\n",
      "F1 Score \t: 0.406420351302241\n",
      "Feature Importances:\n",
      "Feature 1: 0.0004658081540141596\n",
      "Feature 2: 0.0\n",
      "Feature 3: 0.0012121299316346908\n",
      "Feature 4: 0.0\n",
      "Feature 5: 0.005349597189101101\n",
      "Feature 6: 0.0017287745260927106\n",
      "Feature 7: 0.0025633193750964445\n",
      "Feature 8: 0.0\n",
      "Feature 9: 0.0\n",
      "Feature 10: 0.0\n",
      "Feature 11: 0.00022789569858413906\n",
      "Feature 12: 0.000366470959372675\n",
      "Feature 13: 0.0006341490784579505\n",
      "Feature 14: 0.0004232746726651288\n",
      "Feature 15: 0.0006420467228193327\n",
      "Feature 16: 0.0016703049825390237\n",
      "Feature 17: 0.0\n",
      "Feature 18: 0.00018762110442647385\n",
      "Feature 19: 0.0008201175863536985\n",
      "Feature 20: 0.000874035546156173\n",
      "Feature 21: 0.00041831140677244644\n",
      "Feature 22: 0.001100999332487604\n",
      "Feature 23: 0.0\n",
      "Feature 24: 0.0034832237052165554\n",
      "Feature 25: 0.007918437851465739\n",
      "Feature 26: 0.005731546609750814\n",
      "Feature 27: 0.005703930453642478\n",
      "Feature 28: 0.004018203577649589\n",
      "Feature 29: 0.00612043108259767\n",
      "Feature 30: 0.0052548398686697705\n",
      "Feature 31: 0.004578258056774415\n",
      "Feature 32: 0.006514621333395621\n",
      "Feature 33: 0.004984558270527703\n",
      "Feature 34: 0.006229547271566062\n",
      "Feature 35: 0.003434280296274535\n",
      "Feature 36: 0.006980589201022947\n",
      "Feature 37: 0.0022601979295967653\n",
      "Feature 38: 0.009825520441505224\n",
      "Feature 39: 0.0027850296869121037\n",
      "Feature 40: 0.001173998812433173\n",
      "Feature 41: 0.0016965497028507434\n",
      "Feature 42: 0.003947322315837613\n",
      "Feature 43: 0.006615800849202584\n",
      "Feature 44: 0.010085064218795054\n",
      "Feature 45: 0.007636665411869376\n",
      "Feature 46: 0.005493427135612755\n",
      "Feature 47: 0.005897636137050145\n",
      "Feature 48: 0.005227653347541768\n",
      "Feature 49: 0.004909944322297225\n",
      "Feature 50: 0.0007650692496481916\n",
      "Feature 51: 0.07765839152975901\n",
      "Feature 52: 0.007154071542699179\n",
      "Feature 53: 0.002337851456637947\n",
      "Feature 54: 0.004062664632160273\n",
      "Feature 55: 0.023049628969927124\n",
      "Feature 56: 0.001467636795237219\n",
      "Feature 57: 0.02918333866702202\n",
      "Feature 58: 0.018339579944310177\n",
      "Feature 59: 0.0\n",
      "Feature 60: 0.0\n",
      "Feature 61: 0.0032647045763470753\n",
      "Feature 62: 0.0013059452699821072\n",
      "Feature 63: 0.0\n",
      "Feature 64: 0.004472129327707721\n",
      "Feature 65: 0.004527218028762696\n",
      "Feature 66: 0.0015746917974582047\n",
      "Feature 67: 0.0\n",
      "Feature 68: 0.00426338490448957\n",
      "Feature 69: 0.0006399422722026209\n",
      "Feature 70: 0.0\n",
      "Feature 71: 0.00014991993792518526\n",
      "Feature 72: 0.00045519984432242176\n",
      "Feature 73: 0.0\n",
      "Feature 74: 0.002506547375402627\n",
      "Feature 75: 0.0020681521604928783\n",
      "Feature 76: 0.09573760692392409\n",
      "Feature 77: 0.004625256696967997\n",
      "Feature 78: 0.004547817948330903\n",
      "Feature 79: 0.003422087298977945\n",
      "Feature 80: 0.006323896962310478\n",
      "Feature 81: 0.0032324202056047257\n",
      "Feature 82: 0.0022677806583810594\n",
      "Feature 83: 0.0027336869892644225\n",
      "Feature 84: 0.0034760494786332\n",
      "Feature 85: 0.004716408504265238\n",
      "Feature 86: 0.004056782828319591\n",
      "Feature 87: 0.001491593079466751\n",
      "Feature 88: 0.01611994578957045\n",
      "Feature 89: 0.007503475976331155\n",
      "Feature 90: 0.0001599146004535309\n",
      "Feature 91: 0.005175884540476224\n",
      "Feature 92: 0.0005027696667634989\n",
      "Feature 93: 0.005931319126033637\n",
      "Feature 94: 0.016682991656362944\n",
      "Feature 95: 0.00017036356582407418\n",
      "Feature 96: 0.00012850280393587302\n",
      "Feature 97: 0.012687100438036123\n",
      "Feature 98: 0.31561934644883166\n",
      "Feature 99: 0.13624879537380796\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Initialize the decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the decision tree classifier on the training data\n",
    "tree_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "Y_train_pred = tree_clf.predict(X_train)\n",
    "Y_test_pred = tree_clf.predict(X_test)\n",
    "\n",
    "train_metrics_dt, test_metrics_dt = evaluate(Y_train, Y_train_pred, Y_test, Y_test_pred, \"Baseline\", train_metrics, test_metrics)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = tree_clf.feature_importances_\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for i, importance in enumerate(feature_importances):\n",
    "    print(f\"Feature {i+1}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa075c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Training Set Metrics**\n",
      "Accuracy \t: 0.9922518634758729\n",
      "Precision \t: 0.9960466495354813\n",
      "Recall \t\t: 0.9884268340525696\n",
      "F1 Score \t: 0.9922221128285911\n",
      "\n",
      "**Test Set Metrics**\n",
      "Accuracy \t: 0.7738502935420744\n",
      "Precision \t: 0.4484249536751081\n",
      "Recall \t\t: 0.43162901307966706\n",
      "F1 Score \t: 0.4398667070584672\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "feature_importances = tree_clf.feature_importances_\n",
    "\n",
    "# Sort feature importances and select top 20 indices\n",
    "top_20_indices = (-feature_importances).argsort()[:20]\n",
    "\n",
    "# Extract top 20 features\n",
    "X_train_top_20 = X_train.iloc[:, top_20_indices]\n",
    "X_test_top_20 = X_test.iloc[:, top_20_indices]\n",
    "\n",
    "# Train the decision tree classifier on the top 20 features\n",
    "tree_clf_top_20 = DecisionTreeClassifier()\n",
    "tree_clf_top_20.fit(X_train_top_20, Y_train)\n",
    "\n",
    "# Make predictions on the testing data using the model with top 20 features\n",
    "Y_train_pred_top_20 = tree_clf_top_20.predict(X_train_top_20)\n",
    "Y_test_pred_top_20 = tree_clf_top_20.predict(X_test_top_20)\n",
    "\n",
    "# Evaluate the performance of the model with top 20 features\n",
    "train_metrics_dt_top_20, test_metrics_dt_top_20 = evaluate(Y_train, Y_train_pred_top_20, Y_test, Y_test_pred_top_20, \"Top 20 Features\", train_metrics_dt, test_metrics_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9385dae",
   "metadata": {},
   "source": [
    "#### Permutation Importance\n",
    "\n",
    "It assesses the impact of shuffling or permuting the values of individual features on the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "853af286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Training Set Metrics**\n",
      "Accuracy \t: 0.9919576304433111\n",
      "Precision \t: 0.9957501482506425\n",
      "Recall \t\t: 0.9881326010200079\n",
      "F1 Score \t: 0.9919267500246136\n",
      "\n",
      "**Test Set Metrics**\n",
      "Accuracy \t: 0.7712818003913894\n",
      "Precision \t: 0.4454123112659698\n",
      "Recall \t\t: 0.45600475624256837\n",
      "F1 Score \t: 0.45064629847238535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Initialize the decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the decision tree classifier on the training data\n",
    "tree_clf.fit(X_train, Y_train)\n",
    "\n",
    "# Perform permutation importance\n",
    "perm_importance = permutation_importance(tree_clf, X_test, Y_test, n_repeats=10, random_state=42,scoring='f1')\n",
    "\n",
    "# Get feature importances from permutation importance\n",
    "feature_importances_perm = perm_importance.importances_mean\n",
    "\n",
    "# Sort feature importances\n",
    "top_feature_indices_perm = (-feature_importances_perm).argsort()[:20]\n",
    "\n",
    "# Extract top 20 features\n",
    "X_train_top_20_perm = X_train.iloc[:, top_feature_indices_perm]\n",
    "X_test_top_20_perm = X_test.iloc[:, top_feature_indices_perm]\n",
    "\n",
    "# Train the decision tree classifier on the top 20 features\n",
    "tree_clf_top_20_perm = DecisionTreeClassifier()\n",
    "tree_clf_top_20_perm.fit(X_train_top_20_perm, Y_train)\n",
    "\n",
    "# Make predictions on the testing data using the model with top 20 features\n",
    "Y_train_pred_top_20_perm = tree_clf_top_20_perm.predict(X_train_top_20_perm)\n",
    "Y_test_pred_top_20_perm = tree_clf_top_20_perm.predict(X_test_top_20_perm)\n",
    "\n",
    "# Evaluate the performance of the model with top 20 features\n",
    "train_metrics_dt_top_20_perm, test_metrics_dt_top_20_perm = evaluate(Y_train, Y_train_pred_top_20_perm, Y_test, Y_test_pred_top_20_perm, \"Top 20 Features (Permutation Importance)\", train_metrics_dt, test_metrics_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448e9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
