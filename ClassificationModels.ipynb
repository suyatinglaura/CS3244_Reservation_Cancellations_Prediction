{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343dfd66",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "161722c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "\n",
    "sb.set() # set the default Seaborn style for graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415c0f3",
   "metadata": {},
   "source": [
    "## Import Preprocessed Data from the Data Processing step earlier.\n",
    "Note: The preprocessed data are one-hot encoded for categorical variables and scaled for numerical variables (from Data Pre-Processing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ebc6c86d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/basic_model_columns.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19164\\3827313008.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read columns used to build models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data/basic_model_columns.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Columns'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/basic_model_columns.csv'"
     ]
    }
   ],
   "source": [
    "# Read columns used to build models\n",
    "columns = pd.read_csv('Data/basic_model_columns.csv')['Columns'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d94e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/X_train_undersampled_data.csv')[columns]\n",
    "X_test = pd.read_csv('Data/X_test.csv')[columns]\n",
    "Y_train = pd.read_csv('Data/y_train_undersampled_data.csv')\n",
    "Y_test = pd.read_csv('Data/y_test.csv')\n",
    "\n",
    "print(\"Train Set :\", Y_train.shape, X_train.shape)\n",
    "print(\"Test Set  :\", Y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9534fe5",
   "metadata": {},
   "source": [
    "## Attempt 1 - Try to run basic classification models against current preprocessed dataset without any additional tuning (i.e. hyper parameter tuning, feature selection and etc). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441735bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all essential functions from sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set up a dataframe to store the results from different models\n",
    "train_metrics = pd.DataFrame(columns=['Classification Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "test_metrics = pd.DataFrame(columns=['Classification Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702720f",
   "metadata": {},
   "source": [
    "### Logistic Regression Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c02440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logRegModel = LogisticRegression(max_iter=10000, random_state=47).fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Predict the output based on our training and testing dataset\n",
    "Y_train_pred = logRegModel.predict(X_train)\n",
    "Y_test_pred = logRegModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1d7be",
   "metadata": {},
   "source": [
    "#### Plot Confusion Matrix for Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(Y_train, Y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "axes[0].set_title('Train Data Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('Actual Label')\n",
    "\n",
    "sb.heatmap(confusion_matrix(Y_test, Y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "axes[1].set_title('Test Data Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('Actual Label')\n",
    "\n",
    "print(\"Train and Test Data Confusion Matrix:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87543aa4",
   "metadata": {},
   "source": [
    "#### Calculate General Metrics for Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca88fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = {\n",
    "    \"Classification Model\": \"Logistic Regression\",\n",
    "    \"Accuracy\": accuracy_score(Y_train, Y_train_pred),\n",
    "    \"Precision\": precision_score(Y_train, Y_train_pred),\n",
    "    \"Recall\": recall_score(Y_train, Y_train_pred),\n",
    "    \"F1 Score\": f1_score(Y_train, Y_train_pred)\n",
    "}\n",
    "\n",
    "test_metric = {\n",
    "    \"Classification Model\": \"Logistic Regression\",\n",
    "    \"Accuracy\": accuracy_score(Y_test, Y_test_pred),\n",
    "    \"Precision\": precision_score(Y_test, Y_test_pred),\n",
    "    \"Recall\": recall_score(Y_test, Y_test_pred),\n",
    "    \"F1 Score\": f1_score(Y_test, Y_test_pred)\n",
    "}\n",
    "\n",
    "# Save to overall metrics dataframe for comparison later\n",
    "train_metrics = pd.concat([train_metrics, pd.DataFrame.from_records([train_metric])], ignore_index = True)\n",
    "test_metrics = pd.concat([test_metrics, pd.DataFrame.from_records([test_metric])], ignore_index = True)\n",
    "\n",
    "# Calculate general metrics for the train set\n",
    "print(\"**Training Set Metrics**\")\n",
    "print(\"Accuracy \\t:\", train_metric[\"Accuracy\"])\n",
    "print(\"Precision \\t:\", train_metric[\"Precision\"])\n",
    "print(\"Recall \\t\\t:\", train_metric[\"Recall\"])\n",
    "print(\"F1 Score \\t:\", train_metric[\"F1 Score\"])\n",
    "\n",
    "print() # New Line\n",
    "\n",
    "# Calculate general metrics for the test set\n",
    "print(\"**Test Set Metrics**\")\n",
    "print(\"Accuracy \\t:\", test_metric[\"Accuracy\"])\n",
    "print(\"Precision \\t:\", test_metric[\"Precision\"])\n",
    "print(\"Recall \\t\\t:\", test_metric[\"Recall\"])\n",
    "print(\"F1 Score \\t:\", test_metric[\"F1 Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb28b5e",
   "metadata": {},
   "source": [
    "##### Insights based on metrics:\n",
    "There is a decrease in precision, recall and f1 score from the training metrics to the test metrics despite the higher accuracy from training to test. This could suggest some underlying issues such as being able better predict one class over the other. If we look at the confusion matrix, we can actually see that in the test confusion matrix, it is a lot more reliable in predicting class 0 (\"Not Canceled\"), than class 1 (\"Canceled\") based on the high number of false positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b689c4b",
   "metadata": {},
   "source": [
    "### Decision Tree Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8194f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier model from Scikit-Learn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4067fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTreeModel = DecisionTreeClassifier(random_state=47)\n",
    "decisionTreeModel.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the output based on our training and testing dataset\n",
    "Y_train_pred = decisionTreeModel.predict(X_train)\n",
    "Y_test_pred = decisionTreeModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319964d3",
   "metadata": {},
   "source": [
    "#### Plot Confusion Matrix for Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(Y_train, Y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "axes[0].set_title('Train Data Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('Actual Label')\n",
    "\n",
    "sb.heatmap(confusion_matrix(Y_test, Y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "axes[1].set_title('Test Data Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('Actual Label')\n",
    "\n",
    "print(\"Train and Test Data Confusion Matrix:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97994023",
   "metadata": {},
   "source": [
    "#### Calculate General Metrics for Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = {\n",
    "    \"Classification Model\": \"Decision Tree\",\n",
    "    \"Accuracy\": accuracy_score(Y_train, Y_train_pred),\n",
    "    \"Precision\": precision_score(Y_train, Y_train_pred),\n",
    "    \"Recall\": recall_score(Y_train, Y_train_pred),\n",
    "    \"F1 Score\": f1_score(Y_train, Y_train_pred)\n",
    "}\n",
    "\n",
    "test_metric = {\n",
    "    \"Classification Model\": \"Decision Tree\",\n",
    "    \"Accuracy\": accuracy_score(Y_test, Y_test_pred),\n",
    "    \"Precision\": precision_score(Y_test, Y_test_pred),\n",
    "    \"Recall\": recall_score(Y_test, Y_test_pred),\n",
    "    \"F1 Score\": f1_score(Y_test, Y_test_pred)\n",
    "}\n",
    "\n",
    "# Save to overall metrics dataframe for comparison later\n",
    "train_metrics = pd.concat([train_metrics, pd.DataFrame.from_records([train_metric])], ignore_index = True)\n",
    "test_metrics = pd.concat([test_metrics, pd.DataFrame.from_records([test_metric])], ignore_index = True)\n",
    "\n",
    "# Calculate general metrics for the train set\n",
    "print(\"**Training Set Metrics**\")\n",
    "print(\"Accuracy \\t:\", train_metric[\"Accuracy\"])\n",
    "print(\"Precision \\t:\", train_metric[\"Precision\"])\n",
    "print(\"Recall \\t\\t:\", train_metric[\"Recall\"])\n",
    "print(\"F1 Score \\t:\", train_metric[\"F1 Score\"])\n",
    "\n",
    "print() # New Line\n",
    "\n",
    "# Calculate general metrics for the test set\n",
    "print(\"**Test Set Metrics**\")\n",
    "print(\"Accuracy \\t:\", test_metric[\"Accuracy\"])\n",
    "print(\"Precision \\t:\", test_metric[\"Precision\"])\n",
    "print(\"Recall \\t\\t:\", test_metric[\"Recall\"])\n",
    "print(\"F1 Score \\t:\", test_metric[\"F1 Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca992e7",
   "metadata": {},
   "source": [
    "##### Insights based on metrics:\n",
    "In the training metrics, we can see that accuracy, precision, recall and f1 score is close to 1. But however, if we look at the metrics on the testing dataset, we can see there is a sharp drop in almost all the 4 metrics (accuracy, precision, recall, f1 score). This could imply that the default decision tree model (without any tuning) is likely overfitting the training data, including the noise and outliers in the training dataset. Because of the potential overfitting, this means the data will not generalise as well to the testing dataset which it has not seen before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d9def",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1453de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import k nearest neighbour classifier from sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Default k nearest neighbour is n_neighbours = 5 (this can be further tuned in the future)\n",
    "kNeighboursModel = KNeighborsClassifier()\n",
    "kNeighboursModel.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Predict the output based on our training and testing dataset\n",
    "Y_train_pred = kNeighboursModel.predict(X_train)\n",
    "Y_test_pred = kNeighboursModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcbb500",
   "metadata": {},
   "source": [
    "#### Plot Confusion Matrix for K-Nearest Neighbour Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(Y_train, Y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "axes[0].set_title('Train Data Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('Actual Label')\n",
    "\n",
    "sb.heatmap(confusion_matrix(Y_test, Y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "axes[1].set_title('Test Data Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('Actual Label')\n",
    "\n",
    "print(\"Train and Test Data Confusion Matrix:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762ff84",
   "metadata": {},
   "source": [
    "#### Calculate General Metrics for K-Nearest Neighbour Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89077191",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = {\n",
    "    \"Classification Model\": \"K-nearest Neighbour\",\n",
    "    \"Accuracy\": accuracy_score(Y_train, Y_train_pred),\n",
    "    \"Precision\": precision_score(Y_train, Y_train_pred),\n",
    "    \"Recall\": recall_score(Y_train, Y_train_pred),\n",
    "    \"F1 Score\": f1_score(Y_train, Y_train_pred)\n",
    "}\n",
    "\n",
    "test_metric = {\n",
    "    \"Classification Model\": \"K-nearest Neighbour\",\n",
    "    \"Accuracy\": accuracy_score(Y_test, Y_test_pred),\n",
    "    \"Precision\": precision_score(Y_test, Y_test_pred),\n",
    "    \"Recall\": recall_score(Y_test, Y_test_pred),\n",
    "    \"F1 Score\": f1_score(Y_test, Y_test_pred)\n",
    "}\n",
    "\n",
    "# Save to overall metrics dataframe for comparison later\n",
    "train_metrics = pd.concat([train_metrics, pd.DataFrame.from_records([train_metric])], ignore_index = True)\n",
    "test_metrics = pd.concat([test_metrics, pd.DataFrame.from_records([test_metric])], ignore_index = True)\n",
    "\n",
    "# Calculate general metrics for the train set\n",
    "print(\"**Training Set Metrics**\")\n",
    "print(\"Accuracy \\t:\", train_metric[\"Accuracy\"])\n",
    "print(\"Precision \\t:\", train_metric[\"Precision\"])\n",
    "print(\"Recall \\t\\t:\", train_metric[\"Recall\"])\n",
    "print(\"F1 Score \\t:\", train_metric[\"F1 Score\"])\n",
    "\n",
    "print() # New Line\n",
    "\n",
    "# Calculate general metrics for the test set\n",
    "print(\"**Test Set Metrics**\")\n",
    "print(\"Accuracy \\t:\", test_metric[\"Accuracy\"])\n",
    "print(\"Precision \\t:\", test_metric[\"Precision\"])\n",
    "print(\"Recall \\t\\t:\", test_metric[\"Recall\"])\n",
    "print(\"F1 Score \\t:\", test_metric[\"F1 Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33509637",
   "metadata": {},
   "source": [
    "##### Insights based on metrics:\n",
    "The model does not generalise well to new data, as shown by the decreased metrics in the test data. The default K number of neighbours of the model is 5 when we don't input a specify K number, and this could actually result in overfitting due to the small numbers of neighbours and making the model more sensitive to noise/outliers.\n",
    "\n",
    "In addition, our dataset has quite a number of features and K-NN performance is not very good for high dimensional data due to the calculation of distances when there are too many features, the distance between data points will seem to be closer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ba47c",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine (SVM) Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93933ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SVM from Sklearn\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svmModel = SVC(kernel=\"linear\", random_state=47)\n",
    "svmModel.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Predict the output based on our training and testing dataset\n",
    "Y_train_pred = svmModel.predict(X_train)\n",
    "Y_test_pred = svmModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d1bc15",
   "metadata": {},
   "source": [
    "#### Plot Confusion Matrix for Linear Support Vector Machine (SVM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(Y_train, Y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "axes[0].set_title('Train Data Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('Actual Label')\n",
    "\n",
    "sb.heatmap(confusion_matrix(Y_test, Y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "axes[1].set_title('Test Data Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('Actual Label')\n",
    "\n",
    "print(\"Train and Test Data Confusion Matrix:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf5b63",
   "metadata": {},
   "source": [
    "#### Calculate General Metrics for Linear Support Vector Machine (SVM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = {\n",
    "    \"Classification Model\": \"Support Vector Machine\",\n",
    "    \"Accuracy\": accuracy_score(Y_train, Y_train_pred),\n",
    "    \"Precision\": precision_score(Y_train, Y_train_pred),\n",
    "    \"Recall\": recall_score(Y_train, Y_train_pred),\n",
    "    \"F1 Score\": f1_score(Y_train, Y_train_pred)\n",
    "}\n",
    "\n",
    "test_metric = {\n",
    "    \"Classification Model\": \"Support Vector Machine\",\n",
    "    \"Accuracy\": accuracy_score(Y_test, Y_test_pred),\n",
    "    \"Precision\": precision_score(Y_test, Y_test_pred),\n",
    "    \"Recall\": recall_score(Y_test, Y_test_pred),\n",
    "    \"F1 Score\": f1_score(Y_test, Y_test_pred)\n",
    "}\n",
    "\n",
    "# Save to overall metrics dataframe for comparison later\n",
    "train_metrics = pd.concat([train_metrics, pd.DataFrame.from_records([train_metric])], ignore_index = True)\n",
    "test_metrics = pd.concat([test_metrics, pd.DataFrame.from_records([test_metric])], ignore_index = True)\n",
    "\n",
    "# Calculate general metrics for the train set\n",
    "print(\"**Training Set Metrics**\")\n",
    "print(\"Accuracy \\t:\", train_metric[\"Accuracy\"])\n",
    "print(\"Precision \\t:\", train_metric[\"Precision\"])\n",
    "print(\"Recall \\t\\t:\", train_metric[\"Recall\"])\n",
    "print(\"F1 Score \\t:\", train_metric[\"F1 Score\"])\n",
    "\n",
    "print() # New Line\n",
    "\n",
    "# Calculate general metrics for the test set\n",
    "print(\"**Test Set Metrics**\")\n",
    "print(\"Accuracy \\t:\", test_metric[\"Accuracy\"])\n",
    "print(\"Precision \\t:\", test_metric[\"Precision\"])\n",
    "print(\"Recall \\t\\t:\", test_metric[\"Recall\"])\n",
    "print(\"F1 Score \\t:\", test_metric[\"F1 Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c7f832",
   "metadata": {},
   "source": [
    "##### Insights based on metrics:\n",
    "Similar to the logistic regression model, the accuracy improves from the training set to the test set, but precision, recall and f1 score is much lower. One possible theory about this is because the SVM is trained with the kernel=linear parameter, which means its possible that the data is not very linearly seperable. In addition, in the confusion matrix for the test data, we can clearly see that it is able to predict class 0 (\"Not canceled\") very well, but class 1 (\"Canceled\") not as well as shown by the high false positive counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872b636",
   "metadata": {},
   "source": [
    "### Naive Bayes (Gaussian) Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec85d2",
   "metadata": {},
   "source": [
    "Given the nature of our dataset which consist of mixed data types such as numerical features and one hot encoded values, there is no one best fit naive bayes model (Guassian, Bernoulli, Multinomial), but let's try GaussianNB given the scaled numerical features we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gaussian Naive Bayes Classifier from Sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gaussianNBModel = GaussianNB()\n",
    "gaussianNBModel.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Predict the output based on our training and testing dataset\n",
    "Y_train_pred = gaussianNBModel.predict(X_train)\n",
    "Y_test_pred = gaussianNBModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca585d54",
   "metadata": {},
   "source": [
    "#### Plot Confusion Matrix for Naive Bayes (Gaussian) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(Y_train, Y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "axes[0].set_title('Train Data Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('Actual Label')\n",
    "\n",
    "sb.heatmap(confusion_matrix(Y_test, Y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "axes[1].set_title('Test Data Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('Actual Label')\n",
    "\n",
    "print(\"Train and Test Data Confusion Matrix:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ad75b",
   "metadata": {},
   "source": [
    "#### Calculate General Metrics for Naive Bayes (Gaussian) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = {\n",
    "    \"Classification Model\": \"Gaussian Naive Bayes\",\n",
    "    \"Accuracy\": accuracy_score(Y_train, Y_train_pred),\n",
    "    \"Precision\": precision_score(Y_train, Y_train_pred),\n",
    "    \"Recall\": recall_score(Y_train, Y_train_pred),\n",
    "    \"F1 Score\": f1_score(Y_train, Y_train_pred)\n",
    "}\n",
    "\n",
    "test_metric = {\n",
    "    \"Classification Model\": \"Gaussian Naive Bayes\",\n",
    "    \"Accuracy\": accuracy_score(Y_test, Y_test_pred),\n",
    "    \"Precision\": precision_score(Y_test, Y_test_pred),\n",
    "    \"Recall\": recall_score(Y_test, Y_test_pred),\n",
    "    \"F1 Score\": f1_score(Y_test, Y_test_pred)\n",
    "}\n",
    "\n",
    "# Save to overall metrics dataframe for comparison later\n",
    "train_metrics = pd.concat([train_metrics, pd.DataFrame.from_records([train_metric])], ignore_index = True)\n",
    "test_metrics = pd.concat([test_metrics, pd.DataFrame.from_records([test_metric])], ignore_index = True)\n",
    "\n",
    "# Calculate general metrics for the train set\n",
    "print(\"**Training Set Metrics**\")\n",
    "print(\"Accuracy \\t:\", train_metric[\"Accuracy\"])\n",
    "print(\"Precision \\t:\", train_metric[\"Precision\"])\n",
    "print(\"Recall \\t\\t:\", train_metric[\"Recall\"])\n",
    "print(\"F1 Score \\t:\", train_metric[\"F1 Score\"])\n",
    "\n",
    "print() # New Line\n",
    "\n",
    "# Calculate general metrics for the test set\n",
    "print(\"**Test Set Metrics**\")\n",
    "print(\"Accuracy \\t:\", test_metric[\"Accuracy\"])\n",
    "print(\"Precision \\t:\", test_metric[\"Precision\"])\n",
    "print(\"Recall \\t\\t:\", test_metric[\"Recall\"])\n",
    "print(\"F1 Score \\t:\", test_metric[\"F1 Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fffe4a",
   "metadata": {},
   "source": [
    "##### Insights based on metrics:\n",
    "For both train and test metrics, the accuracy is lesser than 55%, which means only 45% of data are incorrectly predicted, indicating that the trained model used is not good for predicting the given output. In addition, another interesting insight is the extremely low precision but high recall, indicating that out of all the actual positives, it was able to predict the positive correctly. But out of teh total positives predicted, a low amount is actually positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae1cff",
   "metadata": {},
   "source": [
    "### Comparing the Different Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_metrics.sort_values(by=['Accuracy'], ascending=True,inplace=True)\n",
    "# train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fc9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = sb.barplot(x=\"Accuracy\", y=\"Classification Model\", data=train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eef83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics.sort_values(by=['Accuracy'], ascending=True,inplace=True)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd9356",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "colormap = plt.cm.get_cmap('tab10')\n",
    "\n",
    "for i, data in enumerate(['Accuracy', 'Precision', 'Recall', 'F1 Score']):\n",
    "    colors = colormap.colors[:len(test_metrics['Classification Model'])]\n",
    "    for index, (model, value) in enumerate(zip(test_metrics['Classification Model'], test_metrics[data])):\n",
    "        axs[i].bar(model, value, color=colors[index], label=model if i == 0 else \"\", edgecolor='k')\n",
    "    axs[i].set_title(data)\n",
    "    axs[i].set_ylabel('Score')\n",
    "    axs[i].set_xlabel('Classification Model')\n",
    "    axs[i].set_xticks(test_metrics['Classification Model'])\n",
    "    axs[i].set_xticklabels(test_metrics['Classification Model'], rotation=45)\n",
    "    for index, value in enumerate(test_metrics[data]):\n",
    "        axs[i].text(index, value, str(round(value, 2)), ha='center', va='bottom')\n",
    "\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='center', bbox_to_anchor=(0.5, -0.03), ncol=len(test_metrics['Classification Model']))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215d8c5",
   "metadata": {},
   "source": [
    "#### Insights From Comparing the Different Models\n",
    "1. The two best models based on Accuracy is Logistic Regression and Support Vector Machine, both with an accuracy of 0.81. In addition, we can actually see that the two of them have similar precision, recall and f1 score as well.\n",
    "\n",
    "2. However, it appears the the logistic regression and SVM model have the lowest recall score, which is how the model correctly identifies positive instances (true positives) from all the actual positive samples in the dataset. Recall is calculated by True Positive / (True Positive + False Negative), a low recall score would imply that we have a lot of false negative, which means we predicted the customer to not cancel the hotel booking, but the actual label is the customer canceled the booking. Recall is very important in our use case, especially since the goal of the project is that we want to find out the customers who will be canceling the booking, as this insight will allow the hotel management to prepare in advance to minimise any disruptions, hence it is crucial to reduce the false negative rate.\n",
    "\n",
    "3. Gaussian Naive Bayes appears to be a bad model for our use case and the given dataset. This is likely due to the mixed data attributes used such as both categorical and numberical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f005b",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "The goal here is to improve on the classification performance, the below are some identified steps that we will perform to try to improve the current results of the basic classification models we have trained above.\n",
    "\n",
    "- Making use of ensembling (bagging and boosting) techniques to improve the classification performance\n",
    "- Feature Engineering and Feature Selection\n",
    "- Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d94fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
