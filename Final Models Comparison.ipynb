{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52859727",
   "metadata": {},
   "source": [
    "## Comparing the Best Classification Models\n",
    "This final notebook is used to compare the best models after we individually tried to do feature engineering, feature selection and hyperparameter tuning to the different classification techniques (e.g. SVM, Random Forest..), to obtain the best model for each classification technique.\n",
    "\n",
    "\n",
    "#### The goal is to compare and select the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac8b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from Utils import plotMetricsGraphComparison # Custom plotting method to compare metrics between models\n",
    "from FeatureSelectionUtils import import_final_selected_features_from_csv # To import the selected features for each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3661ae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set : (20392, 1) (20392, 75)\n",
      "Test Set  : (8176, 1) (8176, 75)\n"
     ]
    }
   ],
   "source": [
    "# Initialise test_metrics dataframe to keep metrics of the different models\n",
    "test_metrics = pd.DataFrame(columns=['Classification Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Load in feature engineered data\n",
    "X_train = pd.read_csv('Data/X_train_engineered.csv')\n",
    "X_test = pd.read_csv('Data/X_test_engineered.csv')\n",
    "Y_train = pd.read_csv('Data/y_train_undersampled_data.csv')\n",
    "Y_test = pd.read_csv('Data/y_test.csv')\n",
    "\n",
    "print(\"Train Set :\", Y_train.shape, X_train.shape)\n",
    "print(\"Test Set  :\", Y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b57c3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Step 1: Load all the best model from each classification models with the best feature selection and hyperparameter tuning done\n",
    "BestModels = {\n",
    "    'random_forest_tuned_and_feature_selected': joblib.load('FinalModels/BestRandomForest.pkl'),\n",
    "    #'decision_tree_tuned_and_feature_selected': joblib.load('FinalModels/BestDecisionTree.pkl'),\n",
    "}\n",
    "\n",
    "# Map each model to their corresponding feature selecction file\n",
    "BestModels_featureselection_files = {\n",
    "    'random_forest_tuned_and_feature_selected': 'FinalModels/Feature_Selection/Best Random Forest_selected_features.csv',\n",
    "    #'decision_tree_tuned_and_feature_selected': 'Data/feature_selection/Best Decision Tree_selected_features.csv',\n",
    "}\n",
    "\n",
    "\n",
    "# Step 2: Load feature selections and evaluate models\n",
    "for model_name, model in BestModels.items():\n",
    "    # Load the corresponding feature selection CSV\n",
    "    feature_selection_csv_path = BestModels_featureselection_files[model_name]\n",
    "    selected_features = import_final_selected_features_from_csv(feature_selection_csv_path)\n",
    "    X_test_feature_selected = X_test[selected_features]\n",
    "    \n",
    "    # Predict the output based on our training and testing dataset\n",
    "    Y_test_pred = model.predict(X_test_feature_selected)\n",
    "    \n",
    "    test_metric = {\n",
    "        \"Classification Model\": model_name,\n",
    "        \"Accuracy\": accuracy_score(Y_test, Y_test_pred),\n",
    "        \"Precision\": precision_score(Y_test, Y_test_pred),\n",
    "        \"Recall\": recall_score(Y_test, Y_test_pred),\n",
    "        \"F1 Score\": f1_score(Y_test, Y_test_pred)\n",
    "    }\n",
    "    \n",
    "    # Save to overall metrics dataframe for comparison later\n",
    "    test_metrics = pd.concat([test_metrics, pd.DataFrame.from_records([test_metric])], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5a2f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest_tuned_and_feature_selected</td>\n",
       "      <td>0.823386</td>\n",
       "      <td>0.582296</td>\n",
       "      <td>0.500595</td>\n",
       "      <td>0.538363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Classification Model  Accuracy  Precision    Recall  \\\n",
       "0  random_forest_tuned_and_feature_selected  0.823386   0.582296  0.500595   \n",
       "\n",
       "   F1 Score  \n",
       "0  0.538363  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c84ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
